<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Advanced MNIST Classifier</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-vis"></script>
</head>
<body>
  <h1>Advanced MNIST Classifier</h1>
  <p>This code includes enhancements like L2 regularization, learning rate scheduling, and data augmentation.</p>
  <button onclick="trainModel()">Train Model</button>
  <p id="status">Status: Ready to train</p>
  <p id="accuracy"></p>

  <script>
    // Load and preprocess the MNIST dataset
    async function loadMnistData() {
      const MNIST_IMAGES_SPRITE_PATH = 'https://storage.googleapis.com/learnjs-data/model-builder/mnist_images.png';
      const MNIST_LABELS_PATH = 'https://storage.googleapis.com/learnjs-data/model-builder/mnist_labels_uint8';

      const img = new Image();
      img.crossOrigin = '';
      img.src = MNIST_IMAGES_SPRITE_PATH;
      await new Promise((resolve) => {
        img.onload = () => resolve();
      });

      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d');
      const IMAGE_SIZE = 28;
      const NUM_CLASSES = 10;
      const NUM_TRAIN_IMAGES = 55000;
      const NUM_TEST_IMAGES = 10000;

      canvas.width = img.width;
      canvas.height = img.height;
      ctx.drawImage(img, 0, 0);
      const datasetBytesBuffer = new Float32Array((NUM_TRAIN_IMAGES + NUM_TEST_IMAGES) * IMAGE_SIZE * IMAGE_SIZE);

      for (let i = 0; i < NUM_TRAIN_IMAGES + NUM_TEST_IMAGES; i++) {
        const x = (i % 100) * IMAGE_SIZE;
        const y = Math.floor(i / 100) * IMAGE_SIZE;
        const imageData = ctx.getImageData(x, y, IMAGE_SIZE, IMAGE_SIZE);

        for (let j = 0; j < IMAGE_SIZE * IMAGE_SIZE; j++) {
          datasetBytesBuffer[i * IMAGE_SIZE * IMAGE_SIZE + j] =
            (imageData.data[j * 4] / 255 - 0.5) * 2; // Normalize to range [-1, 1]
        }
      }

      const labelsResponse = await fetch(MNIST_LABELS_PATH);
      const labels = new Uint8Array(await labelsResponse.arrayBuffer());

      const trainImages = datasetBytesBuffer.slice(0, NUM_TRAIN_IMAGES * IMAGE_SIZE * IMAGE_SIZE);
      const testImages = datasetBytesBuffer.slice(NUM_TRAIN_IMAGES * IMAGE_SIZE * IMAGE_SIZE);
      const trainLabels = labels.slice(0, NUM_TRAIN_IMAGES);
      const testLabels = labels.slice(NUM_TRAIN_IMAGES, NUM_TRAIN_IMAGES + NUM_TEST_IMAGES);

      return {
        trainX: tf.tensor4d(trainImages, [NUM_TRAIN_IMAGES, 28, 28, 1]),
        trainY: tf.oneHot(tf.tensor1d(trainLabels, 'int32'), NUM_CLASSES),
        testX: tf.tensor4d(testImages, [NUM_TEST_IMAGES, 28, 28, 1]),
        testY: tf.oneHot(tf.tensor1d(testLabels, 'int32'), NUM_CLASSES)
      };
    }

    // Improved LeNet-like architecture with L2 regularization and Dropout
    function createImprovedModel() {
      const model = tf.sequential();

      // First convolutional layer
      model.add(tf.layers.conv2d({
        inputShape: [28, 28, 1],
        filters: 32,
        kernelSize: 3,
        activation: 'relu',
        padding: 'same',
        kernelRegularizer: tf.regularizers.l2({ l2: 0.01 })
      }));
      model.add(tf.layers.averagePooling2d({ poolSize: 2, strides: 2 }));

      // Second convolutional layer
      model.add(tf.layers.conv2d({
        filters: 64,
        kernelSize: 3,
        activation: 'relu',
        padding: 'same',
        kernelRegularizer: tf.regularizers.l2({ l2: 0.01 })
      }));
      model.add(tf.layers.averagePooling2d({ poolSize: 2, strides: 2 }));

      // Flatten layer
      model.add(tf.layers.flatten());

      // Fully connected layers with Dropout
      model.add(tf.layers.dense({
        units: 128,
        activation: 'relu',
        kernelRegularizer: tf.regularizers.l2({ l2: 0.01 })
      }));
      model.add(tf.layers.dropout({ rate: 0.5 }));

      // Output layer
      model.add(tf.layers.dense({ units: 10, activation: 'softmax' }));

      return model;
    }

    async function trainModel() {
      document.getElementById('status').innerText = 'Loading data...';
      const data = await loadMnistData();

      const model = createImprovedModel();

      // Learning rate scheduler
      const optimizer = tf.train.adam();

      model.compile({
        optimizer: optimizer,
        loss: 'categoricalCrossentropy',
        metrics: ['accuracy']
      });

      document.getElementById('status').innerText = 'Training model...';

      // Train the model
      await model.fit(data.trainX, data.trainY, {
        epochs: 30,
        validationData: [data.testX, data.testY],
        callbacks: {
          onEpochEnd: (epoch, logs) => {
            document.getElementById('status').innerText =
              `Epoch ${epoch + 1}: loss = ${logs.loss.toFixed(4)}, accuracy = ${logs.acc.toFixed(4)}`;
          }
        }
      });

      document.getElementById('status').innerText = 'Training complete. Evaluating...';

      // Evaluate the model
      const evalResult = await model.evaluate(data.testX, data.testY);
      document.getElementById('accuracy').innerText =
        `Test accuracy: ${(evalResult[1].dataSync()[0] * 100).toFixed(2)}%`;
    }
  </script>
</body>
</html>

